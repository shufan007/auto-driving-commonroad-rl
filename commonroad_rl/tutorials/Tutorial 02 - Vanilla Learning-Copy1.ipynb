{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Tutorial 02 - Vanilla Learning\n",
    "\n",
    "A vanilla learning process starts from scratch with the CommonRoad-RL environment `commonroad-v1` and a designated RL model, [PPO2](https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html) for example. This tutorial explains the following operations:  \n",
    "* how the RL environment and the RL model are configured before the learning begins\n",
    "* how training and testing sessions are carried out differently\n",
    "* how callback functions are created and attached to evaluate the learning\n",
    "* how results are stored for later inspection or reuse\n",
    "\n",
    "Note that all funcionalities/code snippets from this tutorial are provided in `./commonroad_rl/train_model.py`. Please use the full functionality of `train_model.py` to perform your other experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 0. Preparation\n",
    "\n",
    "Please make sure the training and testing data are prepared, otherwise see **Tutorial 01 - Data Preprocessing**. Also, check the followings:\n",
    "* current path is at `commonroad-rl/commonroad_rl`, i.e. one upper layer to the `tutorials` folder\n",
    "* interactive python kernel is triggered from the correct environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Check current path\n",
    "%cd ..\n",
    "%pwd\n",
    "\n",
    "# Check interactive python kernel\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\dev\\mygithub\\AutoDrivingRL\\commonroad-rl\\commonroad_rl\n",
      "work_path:D:\\dev\\mygithub\\AutoDrivingRL\\commonroad-rl\n",
      "base_path:D:\\dev\\mygithub\\AutoDrivingRL\\commonroad-rl\\commonroad_rl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\apps\\\\Anaconda3\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from warnings import warn\n",
    "\n",
    "# Check and set current path\n",
    "current_path = Path().absolute()\n",
    "if current_path.name == \"tutorials\":\n",
    "    base_path = current_path.parent\n",
    "elif current_path.name != \"commonroad_rl\":\n",
    "    warn(\"Expected different path: Please ensure that the pwd is at commonroad-rl/commonroad_rl\")\n",
    "else:\n",
    "    base_path = current_path\n",
    "\n",
    "%cd $base_path\n",
    "%pwd\n",
    "\n",
    "sys.path.append(base_path)\n",
    "work_path = base_path.parent\n",
    "sys.path.append(work_path)\n",
    "\n",
    "print(f\"work_path:{work_path}\")\n",
    "print(f\"base_path:{base_path}\")\n",
    "\n",
    "# Check interactive python kernel\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 1. Load RL environment and model settings\n",
    "\n",
    "Before the learning begins, both the RL environment and the RL model are to be specified. This is done with the files of environment configurations and model hyperparameters respectively.\n",
    "\n",
    "To configure an environment, simply set the values in `./commonroad_rl/gym_commonroad/config.yaml`. Please see the `./commonroad_rl/gym_commonroad/README.md` file for a full description. \n",
    "\n",
    "To adjust the hyperparameters of a model, go to `./commonroad_rl/hyperparams/{model}.yml`. Alternatively, the parameters can be read in and set with easy assignments.\n",
    "\n",
    "Furthermore, we save these settings in the model results folder for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\dev\\mygithub\\AutoDrivingRL\\commonroad-rl\n"
     ]
    }
   ],
   "source": [
    "# %cd $base_path\n",
    "%cd $work_path\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import copy\n",
    "\n",
    "# Read in environment configurations\n",
    "env_configs = {}\n",
    "with open(\"commonroad_rl/gym_commonroad/configs.yaml\", \"r\") as config_file:\n",
    "    env_configs = yaml.safe_load(config_file)[\"env_configs\"]\n",
    "    \n",
    "# Change a configuration directly\n",
    "env_configs[\"reward_type\"] = \"hybrid_reward\"\n",
    "\n",
    "# Save settings for later use\n",
    "log_path = \"commonroad_rl/tutorials/logs/\"\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(log_path, \"environment_configurations.yml\"), \"w\") as config_file:\n",
    "    yaml.dump(env_configs, config_file)\n",
    "\n",
    "# Read in model hyperparameters\n",
    "hyperparams = {}\n",
    "with open(\"commonroad_rl/hyperparams/ppo.yml\", \"r\") as hyperparam_file:\n",
    "    hyperparams = yaml.safe_load(hyperparam_file)[\"commonroad-v1\"]\n",
    "    \n",
    "# Save settings for later use\n",
    "with open(os.path.join(log_path, \"model_hyperparameters.yml\"), \"w\") as hyperparam_file:\n",
    "    yaml.dump(hyperparams, hyperparam_file)\n",
    "    \n",
    "# Remove `normalize` as it will be handled explicitly later\n",
    "if \"normalize\" in hyperparams:\n",
    "    del hyperparams[\"normalize\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 2. Create a training environment\n",
    "Now we are ready to create our customized RL environment `commonroad-v1` with the above configurations.  \n",
    "Based on the API of [OpenAI gym](https://gym.openai.com/docs/), keyword arguments can be appended and passed over to the environment initializer. This comes in handy as we configure training/testing environments and specify data paths.  \n",
    "Moreover, we utilize the [Monitor Wrapper](https://stable-baselines.readthedocs.io/en/master/common/monitor.html) and [Vectorized Environments](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html) from OpenAI Stable Baselines to track and organize the learning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aenum\n",
      "  Downloading aenum-3.1.15-py3-none-any.whl (137 kB)\n",
      "Installing collected packages: aenum\n",
      "Successfully installed aenum-3.1.15\n"
     ]
    }
   ],
   "source": [
    "!pip install aenum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\dev\\mygithub\\AutoDrivingRL\\commonroad-rl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] commonroad_rl.gym_commonroad.commonroad_env - Initialization started\n",
      "D:\\apps\\Anaconda3\\lib\\site-packages\\commonroad\\scenario\\lanelet.py:1263: ShapelyDeprecationWarning: STRtree will be changed in 2.0.0 and will not be compatible with versions < 2.\n",
      "  self._strtee = STRtree(list(self._buffered_polygons.values()))\n",
      "[INFO] commonroad_rl.gym_commonroad.commonroad_env - Training on commonroad_rl/tutorials/data/highD/pickles/problem_train with 35 scenarios\n",
      "[DEBUG] commonroad_rl.gym_commonroad.commonroad_env - Meta scenario path: commonroad_rl/tutorials/data/highD/pickles/meta_scenario\n",
      "[DEBUG] commonroad_rl.gym_commonroad.commonroad_env - Training data path: commonroad_rl/tutorials/data/highD/pickles/problem_train\n",
      "[DEBUG] commonroad_rl.gym_commonroad.commonroad_env - Testing data path: D:\\dev\\mygithub\\AutoDrivingRL\\commonroad-rl/pickles/problem_test\n",
      "[DEBUG] commonroad_rl.gym_commonroad.commonroad_env - Initialization done\n"
     ]
    }
   ],
   "source": [
    "%cd $work_path\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# import commonroad_rl.gym_commonroad\n",
    "from commonroad_rl.gym_commonroad.commonroad_env import CommonroadEnv\n",
    "\n",
    "# Create a Gym-based RL environment with specified data paths and environment configurations\n",
    "meta_scenario_path = \"commonroad_rl/tutorials/data/highD/pickles/meta_scenario\"\n",
    "training_data_path = \"commonroad_rl/tutorials/data/highD/pickles/problem_train\"\n",
    "# training_env = gym.make(\"commonroad-v1\", \n",
    "#                         meta_scenario_path=meta_scenario_path,\n",
    "#                         train_reset_config_path= training_data_path,\n",
    "#                         **env_configs)\n",
    "\n",
    "training_env = CommonroadEnv(meta_scenario_path=meta_scenario_path,\n",
    "                            train_reset_config_path= training_data_path,\n",
    "                            **env_configs)\n",
    "\n",
    "# Wrap the environment with a monitor to keep an record of the learning process\n",
    "info_keywords=tuple([\"is_collision\", \\\n",
    "                     \"is_time_out\", \\\n",
    "                     \"is_off_road\", \\\n",
    "                     \"is_friction_violation\", \\\n",
    "                     \"is_goal_reached\"])\n",
    "training_env = Monitor(training_env, log_path + \"0\", info_keywords=info_keywords)\n",
    "\n",
    "# Vectorize the environment with a callable argument\n",
    "def make_training_env():\n",
    "    return training_env\n",
    "training_env = DummyVecEnv([make_training_env])\n",
    "\n",
    "# Normalize observations and rewards\n",
    "training_env = VecNormalize(training_env, norm_obs=True, norm_reward=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 3. Create a testing environment\n",
    "**Note**: Training environments are used the collect data to update the model. Testing environments are used to evaluate the performance of the model during training without updating the model.\n",
    "\n",
    "Before starting the learning process, it is usual to prepare a testing environment to evaluate the training status during the process. For such, we simply append an additional key to the original environment configurations to create the testing environment. Then, we pass the testing environment to an evaluation callback, which constantly triggers several assessing episodes after a certain number, say 500 or 1000, of training steps.  \n",
    "\n",
    "In addition, we create a customized [callback](https://stable-baselines.readthedocs.io/en/master/guide/callbacks.html) function to save the vectorized and normalized training environment wrapper whenever there is a new best model achieved during learning. This will be useful for later inspection and continual learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] commonroad_rl.gym_commonroad.commonroad_env - Initialization started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\dev\\mygithub\\AutoDrivingRL\\commonroad-rl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] commonroad_rl.gym_commonroad.commonroad_env - Testing on commonroad_rl/tutorials/data/highD/pickles/problem_test with 16 scenarios\n",
      "[DEBUG] commonroad_rl.gym_commonroad.commonroad_env - Meta scenario path: commonroad_rl/tutorials/data/highD/pickles/meta_scenario\n",
      "[DEBUG] commonroad_rl.gym_commonroad.commonroad_env - Training data path: D:\\dev\\mygithub\\AutoDrivingRL\\commonroad-rl/pickles/problem_train\n",
      "[DEBUG] commonroad_rl.gym_commonroad.commonroad_env - Testing data path: commonroad_rl/tutorials/data/highD/pickles/problem_test\n",
      "[DEBUG] commonroad_rl.gym_commonroad.commonroad_env - Initialization done\n"
     ]
    }
   ],
   "source": [
    "%cd $work_path\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "\n",
    "# Append the additional key\n",
    "env_configs_test = copy.deepcopy(env_configs)\n",
    "env_configs_test[\"test_env\"] = True\n",
    "\n",
    "# Create the testing environment\n",
    "testing_data_path = \"commonroad_rl/tutorials/data/highD/pickles/problem_test\"\n",
    "# testing_env = gym.make(\"commonroad-v1\", \n",
    "#                         meta_scenario_path=meta_scenario_path,\n",
    "#                         test_reset_config_path= testing_data_path,\n",
    "#                         **env_configs_test)\n",
    "\n",
    "testing_env = CommonroadEnv(meta_scenario_path=meta_scenario_path,\n",
    "                            test_reset_config_path= testing_data_path,\n",
    "                            **env_configs_test)\n",
    "\n",
    "# Wrap the environment with a monitor to keep an record of the testing episodes \n",
    "log_path_test = \"commonroad_rl/tutorials/logs/test\"\n",
    "os.makedirs(log_path_test, exist_ok=True)\n",
    "\n",
    "testing_env = Monitor(testing_env, log_path_test + \"/0\", info_keywords=info_keywords)\n",
    "\n",
    "# Vectorize the environment with a callable argument\n",
    "def make_testing_env():\n",
    "    return testing_env\n",
    "testing_env = DummyVecEnv([make_testing_env])\n",
    "\n",
    "# Normalize only observations during testing\n",
    "testing_env = VecNormalize(testing_env, norm_obs=True, norm_reward=False, training=False)\n",
    "\n",
    "# Define a customized callback function to save the vectorized and normalized environment wrapper\n",
    "class SaveVecNormalizeCallback(BaseCallback):\n",
    "    def __init__(self, save_path: str, verbose=1):\n",
    "        super(SaveVecNormalizeCallback, self).__init__(verbose)\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def _init_callback(self) -> None:\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        save_path_name = os.path.join(self.save_path, \"vecnormalize.pkl\")\n",
    "        self.model.get_vec_normalize_env().save(save_path_name)\n",
    "        print(\"Saved vectorized and normalized environment to {}\".format(save_path_name))\n",
    "    \n",
    "# Pass the testing environment and customized saving callback to an evaluation callback\n",
    "# Note that the evaluation callback will triggers three evaluating episodes after every 500 training steps\n",
    "save_vec_normalize_callback = SaveVecNormalizeCallback(save_path=log_path)\n",
    "eval_callback = EvalCallback(testing_env, \n",
    "                             log_path=log_path, \n",
    "                             eval_freq=500, \n",
    "                             n_eval_episodes=3, \n",
    "                             callback_on_new_best=save_vec_normalize_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 4. Create a model and start the learning\n",
    "Now we are ready to start a learning process. To such end, we conveniently instantiate a model provided by OpenAI Stable Baselines. For example, we create a PPO2 agent and learn for 5000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {},
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\dev\\mygithub\\AutoDrivingRL\\commonroad-rl\n",
      "Eval num_timesteps=500, episode_reward=-28.71 +/- 104.69\n",
      "Episode length: 24.33 +/- 3.30\n",
      "New best mean reward!\n",
      "Saved vectorized and normalized environment to commonroad_rl/tutorials/logs/vecnormalize.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x201f44107f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%cd $work_path\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create the model together with its model hyperparameters and the training environment\n",
    "model = PPO(env=training_env, **hyperparams)\n",
    "\n",
    "# Start the learning process with the evaluation callback\n",
    "n_timesteps=5000\n",
    "model.learn(n_timesteps, eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "As seen, there are evaluation messages being printed out after every 500 time steps. Additionally, the environment wrapper is saved whenever a best mean reward (of the three evaluating episodes) is obtained.  \n",
    "\n",
    "Now we conclude this tutorial by saving the trained model so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Specify the filename and save the model\n",
    "model.save(\"tutorials/logs/intermediate_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "At this point, a zip file `intermediate_model.zip` and the settings, `environment_configurations.yml` and `model_hyperparameters.yml`, should be found under the designated path, alongside with  `evaluations.npz` created by the evaluation callback, `vecnormalize.pkl` by the saving callback, and `0.monitor.csv` as well as `test/0.monitor.csv` by the monitor wrappers. We shall take advantage of these files in the next tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
